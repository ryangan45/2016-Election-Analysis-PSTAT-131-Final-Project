---
title: "PSTAT 131 Final Project"
author: "Leah Ding (3747821) and Ryan Gan (4227070)"
date: "December 12, 2019"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
always_allow_html: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')

library(knitr)
library(dplyr)
library(tidyverse)
library(ggmap)
library(maps)
library(Rtsne)
library(NbClust)
library(tree)
library(randomForest)
library(maptree)
library(class)
library(reshape2)
library(ggplot2)
library(kableExtra)
library(glmnet)
library(ROCR)
library(cluster)
library(plotmo)
install.packages("gbm")
library(gbm)
```

# Background

**1. What makes voter behavior prediction (and thus election forecasting) a hard problem?**

*Solution:* Voter behavior prediction (and thus election forecasting) can be a hard problem because human behavior, in general, is extremely volatile and unpredictable due to the innumerable amount of variables that could affect ones behavior. Oftentimes, these variables are extremely hard to measure, since they rely on the voter's background and mindset. No two voters are exactly the same, and it is extremely difficult to predict all the factors. 

**2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?**

*Solution:* Nate Silver's approach in 2012 was unique because his prediction focused on the decisiveness in the public, something that other people didn't seem to look at. He used Bayes' Theorem and hierarchical modelling to calculate the probability of the support percentage being over 50% for each individual state on each day. He also incorporated the assumption that polling errors are correlated, and polls in other states can miss in the same direction. This approach allowed him to gather thorough data daily to increase the accuracy of his predictions.

**3. What went wrong in 2016? What do you think should be done to make future predictions better?**

*Solution:* There is a variety of reasons that the predictions were not accurate in 2016, the biggest being that voter behavior is incredibly unpredictable. In this specific election, media greatly influenced the results of the election.Silver tried to take this into account by labeling them as "shocks". However, it was still hard to cateogorize and quantify these variables. In the future, predictions can be made better by considering more voter behavior features such as voting late, or being undecided. Moreover, systematic polling errors can be evaluated more deeply and fixed, incorrectly collected/inaccurate reports on voter behaviors can be updated, and so forth.

# Data
```{r data, message=FALSE, warning=FALSE, include=FALSE}
## read data and convert candidate from string to factor
election.raw <- read_delim("data/election/election.csv", delim = ",") %>% mutate(candidate=as.factor(candidate))

census_meta <- read_delim("data/census/metadata.csv", delim = ";", col_names = FALSE) 
census <- read_delim("data/census/census.csv", delim = ",") 
```

## Election data

```{r, include=FALSE}
kable(election.raw %>% 
  filter(county == "Los Angeles County")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
    full_width=FALSE)
```

**4. Report the dimension of `election.raw` after removing rows with `fips=2000`. Provide a reason for excluding them.**

*Solution:* The dimension of 'election.raw'after removing rows with 'fips=2000' is 18345 x 5. A reason for excluding these rows is that the counties had NA values, which could negatively affect future calculations. 

```{r, include=FALSE}
##Subsetting Election (county-level)
election.raw <- subset(election.raw, !election.raw$fips==2000)
dim(election.raw)
```

## Census data

```{r, include=FALSE}
kable(census %>% head, "html")  %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
  full_width=FALSE) %>%
  scroll_box(width = "100%")
```

```{r, dependson=data, include=FALSE}
kable(census_meta)
```

## Data wrangling
**5. Remove summary rows from `election.raw` data: i.e.,**

*Solution:*
Election Federal data has 32 observations of 5 variables.
Election State data has 298 observations of 5 variables.
Election Data has 18,007 observations of 5 variables.

```{r, include=FALSE}
#Federal level summary into a 'election_federal'
election_federal <- filter(election.raw, is.na(county) & fips =="US")
election_federal
```

```{r, include=FALSE}
#State-level summary into a 'election_state'
election_state <- filter(election.raw, fips != "US" & is.na(county) & fips != "DC" &
  as.character(election.raw$fips) == as.character(election.raw$state))
election_state
```

```{r, include=FALSE}
# Only county level data is to be in 'election'
election <- filter(election.raw, !is.na(county))
election
```


**6. How many named presidential candidates were there in the 2016 election? Draw a bar chart of all votes received by each candidate.**

*Solution*: There were 32 named presidential candidates in the 2016 election.


```{r, include=FALSE}
num.candidate<- unique(election.raw$candidate)
length(num.candidate)
num.candidate
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#bar plot
ggplot(election.raw, aes(x=candidate, y=(votes))) +
  geom_bar(stat="identity",fill="lightblue") +
  scale_y_continuous() + 
  ylab("Votes") +
  xlab("Candidate") +
  coord_flip() +
  ggtitle("Presidential Candidates in 2016 Election")

#log bar plot
ggplot(election.raw, mapping = aes(x=candidate, y=log(votes, base = exp(1)))) +
  geom_bar(stat="identity",fill="lightblue") + 
  scale_y_continuous() + 
  ylab("log of Votes") +
  xlab("Candidate") +
  coord_flip() +
  ggtitle("Presidential Candidates in 2016 Election (log scale)")
```

**7. Create variables `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. **

*Solution:* county_winner data has 3,111 observations of 7 variables and state_winner has 50 observations of 7 variables.
  
```{r echo=FALSE, message=FALSE}
# create county_winner variable
county_winner <- election %>%
group_by(fips) %>%
mutate(total=sum(votes), pct = votes/total) %>%
top_n(1)
# create state_winner variable
state_winner <- election_state %>%
group_by(fips) %>%
mutate(total=sum(votes), pct = votes/total) %>%
top_n(1)
```

# Visualization

```{r, echo=FALSE, message=FALSE}
states = map_data("state")

ggplot(data = states) +
  geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = "white") +
  coord_fixed(1.3) +
  guides(fill=FALSE)  +
  ggtitle("State-Level Map")
```


**8. Draw county-level map and color by county**

```{r echo=FALSE, message=FALSE, warning=FALSE}
counties <- map_data("county")
ggplot(data = counties) + 
  geom_polygon(aes(x = long, y = lat, fill = subregion, group = group), color = "white") + 
  coord_fixed(1.3) +
  guides(fill=FALSE) +
  ggtitle("County-Level Map")
```

**9. Now color the map by the winning candidate for each state.**
  
```{r include = FALSE}
states = map_data("state")
states = mutate(states, fips = state.abb[match(states[,5],tolower(state.name))])
states <- left_join(state_winner,states,by=c("state"="fips"))
```

```{r echo=FALSE}
ggplot(data=states) + 
  geom_polygon(aes(x = long, y = lat, fill = candidate, group = group),
               color = "white") +
  coord_fixed(1.3) + 
  guides(fill=F) +
  ggtitle("Winning Candidate by State")
```

**10. The variable `county` does not have `fips` column. So we will create one by pooling information from `maps::county.fips`.**

```{r include=FALSE, warning=FALSE}
county.fips <- maps::county.fips %>%
  separate(polyname, c("region", "subregion"), sep=",") %>%
  mutate(fips=as.factor(fips))
join_first=left_join(counties, county.fips, by=c("region","subregion"))
join_counties=left_join(join_first, county_winner, by=c("fips"))
```

```{r echo=FALSE}
ggplot(data=join_counties) +
  geom_polygon(aes(x = long, y = lat, fill = candidate, 
  group = group), color = "white") +
  coord_fixed(1.3) +
  ggtitle("Winning Candidate per County")
```

**11. Create a visualization of your choice using census data.** 

*Solution:* We decided to plot the number of White People who voted in each state and the number of Minorities who voted in each state. It clearly shows that minorities are less likely to vote, which could be due to a variety of reasons. 
    
```{r echo=FALSE}
#Mutate data to make the Minority column
census_new <- census %>%
  dplyr::mutate (Minority = Hispanic + Black + Native + Asian + Pacific) #will mutate this data more for number 11
#group by white people
census_white <- aggregate(census_new$White, by=list(census_new$State), FUN = sum, na.rm = TRUE)
#group by minority
census_min <- aggregate(census_new$Minority, by=list(census_new$State), FUN = sum, na.rm = TRUE)

#plot for white
plot_white <- ggplot(census_white, aes(x=census_white$Group.1,y=census_white$x)) + 
  geom_point(color= "dark green", size=0.7) + 
  theme(text = element_text(size=10),
  axis.text.x = element_text(angle=90, hjust=1)) + 
  labs(x="States", y="White Votes") +
  ggtitle("White Voters per State")

#plot for minority
plot_min <- ggplot(census_min, aes(x=census_min$Group.1,y=census_min$x)) + 
  geom_point(color= "brown", size=0.7) + 
  theme(text = element_text(size=10),
  axis.text.x = element_text(angle=90, hjust=1)) + 
  labs(x="States", y="Minority Votes") +
  ggtitle("Minority Voters per State")

plot_white
plot_min
```

**12. In this problem, we aggregate the information into county-level data by computing `TotalPop`-weighted average of each attributes for each county. Create the following variables: Clean census data, Sub-county census data, and County census data**

*Solution:* Census.del data has 72,720 observations of 28 variables. Census.subct data has 72,720 observations of 30 variables. Census.ct data has 3,218 observations of 28 variables.


```{r include=FALSE}
#clean census data
census.del <- census[1:36] %>%
  na.exclude(census) %>%
  mutate(Men = Men/TotalPop*100,
         Employed = Employed/TotalPop*100,
         Citizen = Citizen/TotalPop*100, 
         Minority = Hispanic + Black + Native + Asian + Pacific) %>%
  dplyr::select(-Walk, -PublicWork, -Construction, -Hispanic, 
  -Black, -Native, -Asian, -Pacific, -Women)
```

```{r include=FALSE}
#sub-county
census.subct <- census.del %>%
  group_by(State,County) %>%
  add_tally(TotalPop) %>%
  mutate(CountyTotal = n, Weight = TotalPop/CountyTotal) %>%
  select(-n) %>%
  ungroup
```

```{r include=FALSE}
#county
census.ct <- census.subct %>%
  group_by(State,County) %>%
  summarise_at(vars(Men:CountyTotal), funs(weighted.mean))
head(census.ct)
```

#Dimensionality reduction

**13. Run PCA for both county & sub-county level data.**

*Solution:* We chose to center and scale the features before running PCA because otherwise, most of the principal components that we observed would be driven by a weighted variable that has the largest mean and variance. Thus, rendering it impossible to scale the other variables evenly.

**What are the three features with the largest absolute values of the first principal component?**

*Solution:*

For county level PCA data, the three largest absolute values of the first principal component are IncomePerCap, ChildPoverty, and Poverty. 

For sub-county level PCA data, the three largest absolute values of the first principal component are IncomePerCap, Professional, and Poverty. 

**Which features have opposite signs and what does that mean about the correlation between these features?**

*Solution:* For both PCA datas, many features in each one contain a negative sign. For example, in county level PCA data, Poverty and ChildPoverty both are negative values. This means that the correlation of these features are negative with the features that are positive values (ie. Income and Poverty). The same applies for sub-county level PCA data. If features are the same sign as one another, than they are positively correlated with one another. 

```{r include=FALSE}
#county-level
pc1 <- prcomp(census.ct[3:28], scale=TRUE, center=TRUE)
#county has only 26 PCs
dim(pc1$x)

#sub-county level
pc2 <- prcomp(census.subct[3:30], scale = TRUE, center = TRUE)
#sub-county has only 28 PCs
dim(pc2$x)

#convert to dataframe 
ct.pc <- data.frame(pc1$x[,1:2])
subct.pc <- data.frame(pc2$x[,1:2])

#rotation matrix
ct_rotation <- data.frame(pc1$rotation[,1])
abs(ct_rotation)
ct_rotation

subct_rotation <- data.frame(pc2$rotation[,1])
abs(subct_rotation)
subct_rotation
```

**14. Determine the number of minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses.**

*Solution:* 14 is the minimum number of PCs needed to capture 90% of the variance for the county analysis, and 17 is the minimum number of PCs needed to capture 90% of the variance for the subcounty analysis.

```{r echo=FALSE}
#PC1 uses census.ct for counties
pr.var1 <- pc1$sd^2 #variance explained by each principal component
pve1 <- pr.var1 / sum(pr.var1) #proportion of variance explained by each principal component
cumulative_pve1 <- cumsum(pve1)

#PC2 uses census.subct for subcounties
pr.var2 <- pc2$sd^2 #variance explained by each principal component
pve2 <- pr.var2 / sum(pr.var2) #proportion of variance explained by each principal component
cumulative_pve2 <- cumsum(pve2)

par(mfrow=c(2, 2)) #graphs side by side

#plot of county-level
plot(pve1, type="l", lwd=3, xlab="Principal Component 1", ylab="Counties PVE")
plot(cumulative_pve1, type="l", xlab="Principal Component 1", ylab="County Cumulative
PVE", ylim=c(0, 1), lwd=3)

#plot of sub-county level
plot(pve2, type="l", lwd=3, xlab="Principal Component 2", ylab="Subcounties PVE")
plot(cumulative_pve2, type="l", xlab="Principal Component 2", ylab="Subcounties Cumulative
PVE", ylim=c(0, 1), lwd=3)

#minimum number of PCs needed for 90% of variance
PC1.Num <- which(cumulative_pve1>=0.9)[1]
PC1.Num #county
PC2.Num <- which(cumulative_pve2>=0.9)[1]
PC2.Num #sub-county
```

# Clustering

**15. With `census.ct`, perform hierarchical clustering with complete linkage.**

```{r include=FALSE}
census.ct_clust <- scale(census.ct[3:28])
census.ct_dist <- dist(census.ct_clust, method = "euclidean")
census.ct_hclust <- hclust(census.ct_dist, method = "complete")
clust1 <- cutree(census.ct_hclust, k = 10)
table(clust1)

#reclust with 2 principal components
ct.pc <- data.frame(pc1$x[,1:2])
ct.pc_clust <- scale(ct.pc)
ct.pc_dist <- dist(ct.pc_clust, method = "euclidean")
ct.pc_hclust <- hclust(ct.pc_dist, method = "complete")
clust2 <- cutree(ct.pc_hclust, k = 10)
table(clust2)

#San Mateo 
clust1[which(census.ct$County == "San Mateo")]
clust2[which(census.ct$County == "San Mateo")]
```

**Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate clusters? Comment on what you observe and discuss possible explanations for these observations.**

*Solution:* Before using PCA, clustering decreases form 2584 to 13 in the first 5 clusters and then proceeds to decrease to 1, increase to 14, and decrease to 11. When we recluster with PCA, however, there is a trend of decreasing in the first 3 clusters to increasing from cluster 4 to 5, and then decreasing from cluster 6 to 8, and increasing until the 10th.

From these trends, we believe that San Mateo is placed into Clust 2, group 9. Clust 2, the complete linkage cluster, is more appropriate since a complete link is less susceptible to noise and outliers in contrast to the other method (PCA), which means San Mateo belongs more in Group 9 than in Group 8. In addition, a smaller distance from the mean displays a more appropriate cluster and contains less variance than variables further away from each other. 

# Classification

```{r include=FALSE}
tmpwinner <- county_winner %>% 
  ungroup %>%
  mutate(state = state.name[match(state, state.abb)]) %>%               ## state abbreviations
  mutate_at(vars(state, county), tolower) %>%                           ## to all lowercase
  mutate(county = gsub(" county| columbia| city| parish", "", county))  ## remove suffixes
tmpcensus <- census.ct %>% 
  dplyr::ungroup() %>%
  mutate_at(vars(State, County), tolower)

election.cl <- tmpwinner %>%
  left_join(tmpcensus, by = c("state"="State", "county"="County")) %>% 
  na.omit

## save meta information
election.meta <- election.cl %>% 
  select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election.cl = election.cl %>% 
  select(-c(county, fips, state, votes, pct, total))
```

```{r include=FALSE}
set.seed(10) 
n <- nrow(election.cl)
in.trn <- sample.int(n, 0.8*n) 
trn.cl <- election.cl[ in.trn,]
tst.cl <- election.cl[-in.trn,]
```

```{r include=FALSE}
set.seed(20) 
nfold <- 10
folds <- sample(cut(1:nrow(trn.cl), breaks = nfold, labels = FALSE))
```

```{r include=FALSE}
calc_error_rate = function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
records = matrix(NA, nrow = 3, ncol = 2)
colnames(records) = c("train.error","test.error")
rownames(records) = c("tree","logistic","lasso")
```

**16. Decision tree: train a decision tree by `cv.tree()`.**
    
```{r echo=FALSE}
#training error
trn.clX = trn.cl %>% select(-candidate)
trn.clY = trn.cl$candidate
#test error
tst.clX = tst.cl %>% select(-candidate)
tst.clY = tst.cl$candidate

tree_params <- tree.control(nrow(trn.cl))
#using a 10 fold CV to select tree which minimizes cv misclassification error.
model_tree <- tree(as.factor(candidate) ~ ., data = trn.cl, control = tree_params)
set.seed(1)
cv <- cv.tree(model_tree, folds, FUN = prune.misclass, K = 10)
tree_select <- which(cv$dev == min(cv$dev))

#select the smallest tree size w/ that minimum rate
best_size <- min(cv$size[tree_select]) #best size is 9

#visualize the tree before pruning
draw.tree(model_tree, nodeinfo=TRUE, cex=0.50)
title("Unpruned Tree")
```

```{r, echo = FALSE}
#visualize the tree after pruning
#prune the tree to the size found and plot with draw.tree
pruned_tree <- prune.tree(model_tree, best = best_size,method="misclass")
#summary(pruned_tree)
draw.tree(pruned_tree, nodeinfo=TRUE, cex=0.50)
title("Pruned Tree")

# traning error
pred_pruned <- predict(pruned_tree, trn.clX, type = "class")
pruned_error <- calc_error_rate(pred_pruned, trn.clY)
# test error
pred_tr <- predict(pruned_tree, tst.clX, type = "class")
error_tr <- calc_error_rate(pred_tr, tst.clY)
```

```{r echo=FALSE}
# putting errors into records
records[1,1] <- pruned_error
records[1,2] <- error_tr
kable(records)
```

**Intepret and discuss the results of the decision tree analysis. Use this plot to tell a story about voting behavior in the US.**

*Solution:* From observing, our unpruned decision tree had a 94.1% classification success, which is a good indication for a data set that is as large as ours. Looking closely, we notice that for people who tend to use the transit less, they are more likely to vote for Donald Trump if they have a medium to high income and they're white. This contrasts to Hillary Clinton being popular in counties, minorities, and people who have a low to medium income or are unemployed.

On our pruned decision tree, we notice that it received a total of 93.9% classification success, 0.2% less than our unpruned tree. This could be due to pruning tending to decrease the number of variables the decision tree has, which can affect the accuracy of bigger data sets like ours. But this 0.2% difference is insignificant since it is a very small difference. The pruned decision tree overall provides a more clear visualization since almost all of our previous observations from the unpruned tree still applies. In addition, it also helps us easily observe the different factors that can affect a voter's decision. 


**17. Run a logistic regression to predict the winning candidate in each county.**

```{r include=FALSE}
glm_fit <- glm(factor(candidate)~., data = trn.cl, family = "binomial")
summary(glm_fit)
```

```{r include=FALSE}
set.seed(1)
#glm train
glm_train <- predict(glm_fit, newdata = trn.cl, type = "response")
winner_train <- factor(ifelse(glm_train < 0.5, "Donald Trump", "Hillary Clinton"), levels=c("Donald Trump", "Hillary Clinton"))
result <- factor(ifelse(trn.cl$candidate == "Donald Trump","Donald Trump", "Hillary Clinton"))
table(predicted = winner_train, true = result)

#glm test
glm_test <- predict(glm_fit, newdata = tst.cl, type = "response")
winner_test <- factor(ifelse(glm_test < 0.5, "Donald Trump", "Hillary Clinton"),levels=c("Donald Trump", "Hillary Clinton"))
result2 <- factor(ifelse(tst.cl$candidate == "Donald Trump","Donald Trump", "Hillary Clinton"))
table(predicted = winner_test, true = result2)
```

```{r, echo = FALSE}
#train and test errors
glm_train_error <- calc_error_rate(winner_train, result)
glm_test_error <- calc_error_rate(winner_test, result2)
#print matrix
records[2,1] <- glm_train_error
records[2,2] <- glm_test_error
kable(records)
```


**What are the significant variables? Are the consistent with what you saw in decision tree analysis? Interpret the meaning of a couple of the significant coefficients in terms of a unit change in the variables.**


*Solution:* The significant variables are the following: Citizen, IncomePerCap, Professional, Service, Production, Drive, Carpool, Employed, PrivateWork, Unemployment. These variables are for the most part consistent with what was observed in the decision tree. For example, Employment and Unemployment are significant since they determine what social class the person belongs to and that made a difference in which candidate the voters chose. Voters who Carpooled, like those who used Transit, are probably more liberal and money conscious since they are more environmentally aware and want to save money as well. The Drive category showed a more financially stable group of people who can afford to drive for themselves.

**18.Use the cv.glmnet function from the glmnet library to run K-fold cross validation and select the best regularization parameter for the logistic regression with LASSO penalty.**

```{r}
y.trn <- ifelse(trn.cl[,1] == "Donald Trump", 0, 1) 
x.trn <- model.matrix(candidate~. , trn.cl)[,-1] 

set.seed(1) 
cv_lasso <- cv.glmnet(lambda = c(1, 5, 10, 50) * 1e-4, x.trn, y.trn, foldid = folds, alpha =1, family = "binomial") 
plot(cv_lasso) 
bestlambda <- cv_lasso$lambda.min #1e-04
abline(v = log(bestlambda), col="navy", lwd = 3, lty = 2) 

lasso_mod <- glmnet(x.trn, y.trn, alpha = 1, family = "binomial") 
coeff <- predict(lasso_mod, type = "coefficients", s = bestlambda) 

plot.glmnet(lasso_mod, xvar="lambda", label = TRUE) 

x.tst = model.matrix(candidate~., tst.cl)[,-1] 

set.seed(1) 
lasso_train_pred = predict(lasso_mod, newx = x.trn, s = bestlambda) 
lasso_train = ifelse(lasso_train_pred < 0.5, "Donald Trump","Hillary Clinton") 
las_train_err = calc_error_rate(as.tibble(lasso_train), trn.cl[,1]) 

lasso_test_pred = predict(lasso_mod, newx = x.tst, s = bestlambda) 
lasso_test = ifelse(lasso_test_pred < 0.5, "Donald Trump","Hillary Clinton") 
las_test_err = calc_error_rate(as.tibble(lasso_test), tst.cl[,1]) 

#non-zero coeff
#as.matrix(coeff)
```

```{r, echo = FALSE}
#matrix 
records[3,1] <- las_train_err
records[3,2] <- las_test_err
kable(records)
```

**What is the optimal value of λ in cross validation? What are the non-zero coefficients in the LASSO regression for the optimal value of λ? How do they compare to the unpenalized logistic regression? Save training and test errors to the records variable.**

*Solution:* The optimal λ value in cross validation is 0.0001. The non-zero coefficients in the LASSO regression for the optimal value of λ were all of the variables excluding Transit, Self-Employed, and Minority because many of the variables in our dataset affect the outcome. The LASSO regression is used for data sets with not enough data, which has high variance estimates. This is in contrast to logistic regression, which is better for big data. We use the LASSO regression to use the shrinkage method and reduce the variance. However, because our data set is large and many of our variables influence our outcome, they don't have a coefficient of zero. The largest non-zero coefficients that we had were Men, Office, MeanCommute, and PrivateWork, which are the same variables with the highest estimates from the logistic regression.  

In contrast to the unpenalized logistic regression, the LASSO regression has less variables to work with because some of the variables equal 0.  

In conclusion, the LASSO and logistic regression fits look very similar. The errors are so close to each other since there is already enough data to estimate the coefficients to high accuracy. This means the LASSO regression does not provide any extra insight in contrast to a different scenario with a smaller data set. 

**19. Compute ROC curves for the decision tree, logistic regression and LASSO logistic regression using predictions on the test data.**

```{r include=FALSE}
#mutating candidates 0 = Hillary, 1 = Trump
trn.m = trn.cl %>%
  mutate(candidate = as.factor(ifelse(candidate == 0, "Hillary Clinton", "Donald Trump")))
```

```{r echo=FALSE}
#decision tree
pred_tree <- prediction(as.numeric(pred_pruned), as.numeric(trn.cl$candidate))
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
plot(perf_tree, col = "maroon", lwd = 3, main = "ROC Curves")
abline(0,1)

#logistic 
pred_log <- prediction(as.numeric(glm_train), as.numeric(trn.cl$candidate))
perf_log <- performance(pred_log, measure = 'tpr', x.measure = 'fpr')
plot(perf_log, add = TRUE, col = "forestgreen", lwd = 9)
abline(0,1)

#lasso
pred_lasso <- prediction(as.numeric(glm_train), as.numeric(trn.cl$candidate))
perf_lasso <- performance(pred_log, measure = 'tpr', x.measure = 'fpr')
plot(perf_lasso, add = TRUE, col = "lightblue", lwd = 3)
abline(0,1)
legend("bottomright", legend = c("decision tree", "log", "lasso"), col = c("maroon","forestgreen", "lightblue"), lty = 1, cex = 0.7)
```

```{r include=FALSE}
auc_tree <- performance(pred_tree, "auc")@y.values #0.8297726
auc_tree
auc_log <- performance(pred_log, "auc")@y.values  #0.9528419
auc_log
auc_lasso <- performance(pred_lasso, "auc")@y.values  #0.9528419
auc_lasso
```

**Based on your classification results, discuss the pros and cons of the various methods.  Are the different classifiers more appropriate for answering different kinds of questions about the election?**

*Solution:* From the ROC curves, the logistic regression and LASSO methods give the highest true positive rate since both curves hug the upper left corner the most. We also calculated the AUC (Area Under Curve) for each method to accurately determine which method gives the best predictions. The AUC for the decision tree method is the lowest at 0.8297726 while the AUC values for logistic regression and the LASSO method are the highest and the exact same value at 0.9528419. This confirms that the logistic regression and LASSO methods are the best predictive models.

We noticed the decision tree fails to address understanding voter behavior (which entails narrowing down the most variables) as well as the LASSO and logistic regression model, making these two classifiers more appropriate. The LASSO was able to do so with our large data set and the logistic regression model helped us identify the best candidate in the election for each variable group through separation. This gives us more insight on voter behavior. 

# Taking it further

**20. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does/doesn’t seems reasonable based on your understanding of these methods, propose possible directions (collecting additional data, domain knowledge, etc). **

*Solution:* For this question, we decided to explore the classification methods: boosting, bagging, and random forests. Our goal is to fit these methods to our data and compare their respective final errors. The method with the smallest error will be the best model. 



**Boosting**

```{r boosting, echo=FALSE}
set.seed(1)
#Trump = 0, Clinton = 1
true_test <- as.numeric(ifelse(tst.cl$candidate == "Donald Trump", 0,1))
boost.elect.cl <- gbm(ifelse(candidate == "Donald Trump", 0,1)~., data = trn.cl, 
    distribution = "bernoulli", n.trees = 800) 
#summary(boost.elect.cl, main = "Boosting Election.cl")

par(mfrow = c(1,2))
plot(boost.elect.cl, i = "Minority", ylab= "y(Minority)")
```

```{r echo = FALSE}
plot(boost.elect.cl, i = "SelfEmployed", ylab= "y(SelfEmployed)")

yhat.boost <- predict(boost.elect.cl, newdata = tst.cl, n.trees = 800, type = "response")
```

```{r echo = FALSE}
#confusion matrix
boost.error <- table(pred = yhat.boost, truth = true_test)
test.boost.error <- 1 - sum(diag(boost.error))/sum(boost.error) #0.9983713
record1 <- matrix(c(test.boost.error, test.boost.error), nrow = 1, ncol = 1)
colnames(record1) = c("test.error")
rownames(record1) = c("boosting")
kable(record1)
```


We first used the boosting method. In doing so, we received an error of 0.9983713, a significantly high error. This may be because boosting is more fit for smaller data sets and decision trees. From our relative influence graph, we are told that the variables Minority, Self-Employed, and Child Poverty are the more influential variables. However, this does not display all of our variables and will not provide enough information. It is also inconsistent with our random forest and the Self-Employment graph that we plotted below, in which the Self-Employed actually decreases.

In conclusion, the boosting method is not great at fitting our large data set compared to the logistic regression and other previously used methods. Its plots and graphs don't provide us with enough information on how to interpret the importance of these variables to our data and voter behavior.


**Bagging**

Next, we tested the bagging method. We observed an error of 0.04885, which is lower than the boosting method and the better method so far. We believe the small error occurred because bagging involves using large unpruned decision trees like our data set. However, this method only uses 2/3 of the total date, which may not provide as much insight on the importance of variance. But it reduces variance and is useful for larger data sets.

In conclusion , the bagging classification method is not as great as the logistic regression and decision tree. But it is better than the boosting method in this scenario. 

```{r bagging, echo=FALSE}
set.seed(1)
trn.cl$candidate <- factor(trn.cl$candidate)
bag.elect.cl <- randomForest(candidate~., data = trn.cl, mtry = 10, importance = TRUE)

plot(bag.elect.cl)
legend("center", colnames(bag.elect.cl$err.rate), col = 1:4, cex = 0.8, fill = 1:4)

bag.elect.cl <- randomForest(candidate~., data = trn.cl, mtry = 10, ntree = 700, importance = TRUE)
yhat.bag <- predict(bag.elect.cl, newdata = tst.cl)

#confusion matrix
bag.error <- table(pred = yhat.bag, truth = true_test)
test.bag.error <- 1 - sum(diag(bag.error))/sum(bag.error)   #0.04885993
record1 <- matrix(c(test.boost.error, test.bag.error), nrow = 2, ncol = 1)
colnames(record1) = c("test.error")
rownames(record1) = c("boost error","bag error")
kable(record1)
```

**Random Forest**

Finally,  we computed random forest by creating more trees. We get an error of 0.04885, which is the same as the bagging method. This is a good result for a large data set despite it being small. Diving into the tree, we can see from the Variance Importance charts that the variables Transit, White and Minority play the biggest roles in decreasing the Gini impurity, which is one of the main goals of this method. Following closely are the variables County Total and Professional. This result is held up by previous methods in this project such as the decision trees. This shows the same variables with with the most important towards the top of the trees and the rest branching downwards.

In relation to the election, this makes sense since a voter's demographic as well as social-economic status played a role in their choice of presidential candidate. For example, most white people voted for Donald Trump while most minorities voted for Hillary Clinton.

In conclusion, the random forest tree is an informative method that helps identify strong predictors in the data set despite it being prone to over-fitting similar to the logistical regression model.

```{r random forest, echo=FALSE}
set.seed(1)
options(stringsAsFactors = FALSE)
true_test <- as.numeric(ifelse(tst.cl$candidate == "Donald Trump", 0,1))

#glimpse(election) #18,007 observations, 5 variables
#change candidate to factor
trn.cl$candidate <- factor(trn.cl$candidate)
rf.election <- randomForest(candidate~., data = trn.cl, mtry = 3, ntree = 1000, importance = TRUE)
plot(rf.election)

yhat.rf <- predict(rf.election, newdata = tst.cl)

#importance(rf.election)
varImpPlot(rf.election, sort = TRUE, main = "Variable Importance for Random Forest Election", n.var = 5)
```

```{r, echo = FALSE}
#tree, log reg, and lasso records
kable(records)

#create matrix
rf.error <- table(pred = yhat.rf, truth = true_test)
test.rf.error <- 1 - sum(diag(rf.error))/sum(rf.error)  #0.04885993

record1 <- matrix(c(test.boost.error, test.bag.error, test.rf.error), nrow = 3, ncol = 1)
colnames(record1) = c("test.error")
rownames(record1) = c("boosting", "bagging","random forest")
kable(record1)
```


From this question, we observed that of the three additional methods that we tested, boosting is the worst with an error close to 1. In contrast, bagging and random forest both have errors that were similar to the test errors of our previously used methods: decision tree, logistic regression, and LASSO.









